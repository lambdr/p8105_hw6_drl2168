---
title: "Homework 6"
author: "Derek Lamb"
date: "`r Sys.Date()`"
output: github_document
---
### Load packages
```{r load packages and default options, message = FALSE}
# Load packages
library(tidyverse)
library(broom)


# Set default figure options
knitr::opts_chunk$set(
  fig.width = 6,
  out.width = "90%"
)

theme_set(theme_bw() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1
I imported the data using similar code to that which I wrote in HW5, but with a few additional cleaning steps for this problem. The final data set considers only 5 variables: the city & state of the homicide, whether the case was solved, and victim demographics (age, race, sex).
```{r import homicide data, message = FALSE, warning = FALSE}
df_homicide <- read_csv("data/homicide-data.csv", 
                        na = "Unknown") |> 
  mutate(
    state = str_to_upper(state),
    city_state = str_c(city, ", ", state),
    solved = case_when(
      disposition == "Closed by arrest" ~ 1,
      disposition != "Closed by arrest" ~ 0
    )
    ) |> 
  filter(!city_state %in% c("Tulsa, AL", "Dallas, TX", "Phoenix, AZ", "Kansas City, MO"),
         victim_race %in% c("Black", "White")) |> 
  select(city_state, victim_age, victim_sex, victim_race, solved)
```

Now I will fit a logistic regression model to predict the odds of cases being solved by victim demographics.

```{r logistic regression on baltimore}
bmore_model = 
  df_homicide |> 
  filter(city_state == "Baltimore, MD") |> 
  glm(solved ~ victim_age + victim_race + victim_sex,
      family = binomial,
      data = _)
```

This model is stored within R. I will use `tidy()` from the `broom` package to examine it further. Calculating the adjusted odds ratios and confidence intervals in a tidy format was challenging, so I created the function below to add the values of interest to an input dataframe.

```{r function for adding or and ci to df}
or_ci = function(df){
  df = 
    df |> 
    mutate(
      ci_lower = exp(estimate - 1.96*std.error),
      or = exp(estimate),
      ci_upper = exp(estimate + 1.96*std.error)
    )
  
  return(df)
}
```

Now I will apply the `or_ci` function to the logistic regression model for Baltimore, MD.
```{r bmore or}
bmore_or_sex = 
  bmore_model |> 
  tidy() |> 
  or_ci() |> 
  filter(term == "victim_sexMale") |> 
  select(ci_lower:ci_upper)
```

The adjusted odds ratio for sex on homicide is `r bmore_or_sex |> pull(or) |> round(digits = 2)` (95% CI: `r bmore_or_sex |> pull(ci_lower) |> round(digits = 2)`, `r bmore_or_sex |> pull(ci_upper) |> round(digits = 2)`), this means that when the homicide victim is male in Baltimore, MD, the case is 57% less likely to be solved than when the victim is female, holding other demographic variables constant.


To apply this approach to the entire `df_homicide` dataframe, I will create a second function to do the processing steps outlined in the previous code chunk.

```{r or cleaning function}
clean_or = function(df){
  df = 
    df |> 
    tidy() |> 
    or_ci() |> 
    filter(term == "victim_sexMale") |> 
    select(ci_lower:ci_upper)
  
  return(df)
}
```

To check that this still works, I will again apply it to the Baltimore model.

```{r check clean_or function}
bmore_model |> 
  clean_or()
```

It still works. Now I will do this for the entire dataset, using `map` functions.

```{r ors for each df}
df_or_homicide = 
  df_homicide |> 
  group_by(city_state) |> 
  nest() |> 
  mutate(
    models = map(data, \(df) glm(solved ~ victim_age + victim_race + victim_sex,
                                family = binomial,
                                data = df)),
    odds_ratio = map(models, clean_or)
  ) |> 
  unnest(odds_ratio) |> 
  select(city_state, ci_lower:ci_upper) |> 
  ungroup()
```

```{r create adj or plot}
df_or_homicide |> 
  mutate(city_state = fct_reorder(city_state, or)) |> 
  ggplot(aes(x = or, y = city_state)) + 
  geom_point() + 
  geom_errorbarh(aes(xmin = ci_lower, xmax = ci_upper)) + 
  labs(
      title = "Adjusted Odds Ratios for Sex on Homicide Closure",
      x = " Adjusted Odds Ratio",
      y = "City & State"
  )
```

In most cities, the point estimate for the adjusted odds ratio is below 1, suggesting that homicide cases with male victims are less likely to be solved than those with female victims, holding other variables constant. In some cities, such as New York City and Baton Rouge, the odds ratio looks to be much lower than 0.5. There are three cities with ORs appreciably above 1, suggesting that homicide cases with male victims are more likely to be solved than female victims, holding other variables constant. These three cities have wide CIs that cross 1, however. Indeed, over half of these CIs cross 1, bringing the relationship between victim sex and homicide closure into question generally, though it is clear in some cities.

# Problem 2
I will pull in the data from the Central Park weather station.
```{r import noaa data}
df_weather = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```

Here is a bootstrapping function that I think will be useful.
```{r bootstrap function}
boot_samp = function(df){
  sample_frac(df, replace = TRUE)
}
```

This is what I want to happen, just 5000 times.
```{r prototype bootstrap}
df_weather |> 
  boot_samp() |> 
  lm(tmax ~ tmin + prcp, data =_) |> 
  tidy()
```

I created the dataframe I needed with 5000 samples of the weather dataframe.
```{r create bootstrap df}
df_bootstrap = 
  tibble(n_strap = 1:5000) |> 
  mutate(
    samp = map(n_strap, \(i) boot_samp(df_weather))
  )
```

I then applied a linear model to each sample, and then used both the `tidy` and `glance` functions from `broom` on the model. Both of these functions output a `p.value` column within their dataframes, so I needed to unnest and extract data one at a time. 

Some of the values of $\hat{\beta_1}*\hat{\beta_2}$ were negative, and therefore the logarithm of these values is not real. I therefore decided to apply the absolute value function to these values in order to visualize them.
```{r regression on bootstrap df}
df_bootstrap =
  df_bootstrap |> 
  mutate(model = map(samp, \(df) lm(tmax ~ tmin + prcp, data = df)),
         model_glance = map(model, glance),
         model_tidy = map(model, tidy)) |> 
  unnest(model_glance) |> 
  select(n_strap, r.squared, model_tidy) |> 
  unnest(model_tidy) |> 
  select(n_strap, r.squared, term, estimate) |> 
  filter(term != "(Intercept)") |> 
  pivot_wider(
    names_from = term,
    values_from = estimate) |> 
  mutate(log_beta = log(abs(tmin*prcp))) |> 
  select(r.squared,log_beta)
```

I will now construct histograms for these two estimated parameters.
```{r histograms of r.squared and log_beta}
df_bootstrap |> 
  ggplot(aes(x = r.squared)) +
  geom_histogram() + 
  labs(
    title = "Distribution of Bootstrap R-squared",
    x = "R-squared",
    y = "Frequency"
  )

df_bootstrap |> 
  ggplot(aes(x = log_beta)) +
  geom_histogram() + 
  labs(
    title = "Distribution of Bootstrap Log Beta Product",
    x = "Log(beta1 * beta2",
    y = "Frequency"
  ) 
```

The distribution of $R^2$ looks close to normal, with a mean around 0.92. The distribution of $log(|\hat{\beta_1}*\hat{\beta_2}|)$ is heavily left skewed, with a mode around -5. I will use quantiles to obtain confidence intervals of these estimates.

```{r cis for estimates}
df_bootstrap |> 
  pivot_longer(
    r.squared:log_beta,
    names_to = "parameter",
    values_to = "estimate"
  ) |> 
  group_by(parameter) |> 
  summarize(
    ci_lower = quantile(estimate, 0.025),
    ci_upper = quantile(estimate, 0.975)
  ) |> 
  knitr::kable(col.names = c("Parameter","CI Lower Bound", "CI Upper Bound"),
               digits = 3)
```

