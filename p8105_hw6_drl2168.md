Homework 6
================
Derek Lamb
2023-11-21

### Load packages

``` r
# Load packages
library(tidyverse)
library(broom)


# Set default figure options
knitr::opts_chunk$set(
  fig.width = 6,
  out.width = "90%"
)

theme_set(theme_bw() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1

I imported the data using similar code to that which I wrote in HW5, but
with a few additional cleaning steps for this problem. The final data
set considers only 5 variables: the city & state of the homicide,
whether the case was solved, and victim demographics (age, race, sex).

``` r
df_homicide <- read_csv("data/homicide-data.csv", 
                        na = "Unknown") |> 
  mutate(
    state = str_to_upper(state),
    city_state = str_c(city, ", ", state),
    solved = case_when(
      disposition == "Closed by arrest" ~ 1,
      disposition != "Closed by arrest" ~ 0
    )
    ) |> 
  filter(!city_state %in% c("Tulsa, AL", "Dallas, TX", "Phoenix, AZ", "Kansas City, MO"),
         victim_race %in% c("Black", "White")) |> 
  select(city_state, victim_age, victim_sex, victim_race, solved)
```

Now I will fit a logistic regression model to predict the odds of cases
being solved by victim demographics.

``` r
bmore_model = 
  df_homicide |> 
  filter(city_state == "Baltimore, MD") |> 
  glm(solved ~ victim_age + victim_race + victim_sex,
      family = binomial,
      data = _)
```

This model is stored within R. I will use `tidy()` from the `broom`
package to examine it further. Calculating the adjusted odds ratios and
confidence intervals in a tidy format was challenging, so I created the
function below to add the values of interest to an input dataframe.

``` r
or_ci = function(df){
  df = 
    df |> 
    mutate(
      ci_lower = exp(estimate - 1.96*std.error),
      or = exp(estimate),
      ci_upper = exp(estimate + 1.96*std.error)
    )
  
  return(df)
}
```

Now I will apply the `or_ci` function to the logistic regression model
for Baltimore, MD.

``` r
bmore_or_sex = 
  bmore_model |> 
  tidy() |> 
  or_ci() |> 
  filter(term == "victim_sexMale") |> 
  select(ci_lower:ci_upper)
```

The adjusted odds ratio for sex on homicide is 0.43 (95% CI: 0.32,
0.56), this means that when the homicide victim is male in Baltimore,
MD, the case is 57% less likely to be solved than when the victim is
female, holding other demographic variables constant.

To apply this approach to the entire `df_homicide` dataframe, I will
create a second function to do the processing steps outlined in the
previous code chunk.

``` r
clean_or = function(df){
  df = 
    df |> 
    tidy() |> 
    or_ci() |> 
    filter(term == "victim_sexMale") |> 
    select(ci_lower:ci_upper)
  
  return(df)
}
```

To check that this still works, I will again apply it to the Baltimore
model.

``` r
bmore_model |> 
  clean_or()
```

    ## # A tibble: 1 × 3
    ##   ci_lower    or ci_upper
    ##      <dbl> <dbl>    <dbl>
    ## 1    0.325 0.426    0.558

It still works. Now I will do this for the entire dataset, using `map`
functions.

``` r
df_or_homicide = 
  df_homicide |> 
  group_by(city_state) |> 
  nest() |> 
  mutate(
    models = map(data, \(df) glm(solved ~ victim_age + victim_race + victim_sex,
                                family = binomial,
                                data = df)),
    odds_ratio = map(models, clean_or)
  ) |> 
  unnest(odds_ratio) |> 
  select(city_state, ci_lower:ci_upper) |> 
  ungroup()
```

``` r
df_or_homicide |> 
  mutate(city_state = fct_reorder(city_state, or)) |> 
  ggplot(aes(x = or, y = city_state)) + 
  geom_point() + 
  geom_errorbarh(aes(xmin = ci_lower, xmax = ci_upper)) + 
  labs(
      title = "Adjusted Odds Ratios for Sex on Homicide Closure",
      x = " Adjusted Odds Ratio",
      y = "City & State"
  )
```

<img src="p8105_hw6_drl2168_files/figure-gfm/create adj or plot-1.png" width="90%" />

In most cities, the point estimate for the adjusted odds ratio is below
1, suggesting that homicide cases with male victims are less likely to
be solved than those with female victims, holding other variables
constant. In some cities, such as New York City and Baton Rouge, the
odds ratio looks to be much lower than 0.5. There are three cities with
ORs appreciably above 1, suggesting that homicide cases with male
victims are more likely to be solved than female victims, holding other
variables constant. These three cities have wide CIs that cross 1,
however. Indeed, over half of these CIs cross 1, bringing the
relationship between victim sex and homicide closure into question
generally, though it is clear in some cities.

# Problem 2

I will pull in the data from the Central Park weather station.

``` r
df_weather = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```

    ## using cached file: /Users/Derek/Library/Caches/org.R-project.R/R/rnoaa/noaa_ghcnd/USW00094728.dly

    ## date created (size, mb): 2023-09-28 10:20:09.047204 (8.524)

    ## file min/max dates: 1869-01-01 / 2023-09-30

Here is a bootstrapping function that I think will be useful.

``` r
boot_samp = function(df){
  sample_frac(df, replace = TRUE)
}
```

This is what I want to happen, just 5000 times.

``` r
df_weather |> 
  boot_samp() |> 
  lm(tmax ~ tmin + prcp, data =_) |> 
  tidy()
```

    ## # A tibble: 3 × 5
    ##   term        estimate std.error statistic   p.value
    ##   <chr>          <dbl>     <dbl>     <dbl>     <dbl>
    ## 1 (Intercept)  7.62      0.203       37.5  7.98e-127
    ## 2 tmin         1.06      0.0145      73.1  6.24e-219
    ## 3 prcp        -0.00759   0.00189     -4.02 7.08e-  5

I created the dataframe I needed with 5000 samples of the weather
dataframe.

``` r
df_bootstrap = 
  tibble(n_strap = 1:5000) |> 
  mutate(
    samp = map(n_strap, \(i) boot_samp(df_weather))
  )
```

I then applied a linear model to each sample, and then used both the
`tidy` and `glance` functions from `broom` on the model. Both of these
functions output a `p.value` column within their dataframes, so I needed
to unnest and extract data one at a time.

Some of the values of $\hat{\beta_1}*\hat{\beta_2}$ were negative, and
therefore the logarithm of these values is not real. I therefore decided
to apply the absolute value function to these values in order to
visualize them.

``` r
df_bootstrap =
  df_bootstrap |> 
  mutate(model = map(samp, \(df) lm(tmax ~ tmin + prcp, data = df)),
         model_glance = map(model, glance),
         model_tidy = map(model, tidy)) |> 
  unnest(model_glance) |> 
  select(n_strap, r.squared, model_tidy) |> 
  unnest(model_tidy) |> 
  select(n_strap, r.squared, term, estimate) |> 
  filter(term != "(Intercept)") |> 
  pivot_wider(
    names_from = term,
    values_from = estimate) |> 
  mutate(log_beta = log(abs(tmin*prcp))) |> 
  select(r.squared,log_beta)
```

I will now construct histograms for these two estimated parameters.

``` r
df_bootstrap |> 
  ggplot(aes(x = r.squared)) +
  geom_histogram() + 
  labs(
    title = "Distribution of Bootstrap R-squared",
    x = "R-squared",
    y = "Frequency"
  )
```

    ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.

<img src="p8105_hw6_drl2168_files/figure-gfm/histograms of r.squared and log_beta-1.png" width="90%" />

``` r
df_bootstrap |> 
  ggplot(aes(x = log_beta)) +
  geom_histogram() + 
  labs(
    title = "Distribution of Bootstrap Log Beta Product",
    x = "Log(beta1 * beta2",
    y = "Frequency"
  ) 
```

    ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.

<img src="p8105_hw6_drl2168_files/figure-gfm/histograms of r.squared and log_beta-2.png" width="90%" />

The distribution of $R^2$ looks close to normal, with a mean around
0.92. The distribution of $log(|\hat{\beta_1}*\hat{\beta_2}|)$ is
heavily left skewed, with a mode around -5. I will use quantiles to
obtain confidence intervals of these estimates.

``` r
df_bootstrap |> 
  pivot_longer(
    r.squared:log_beta,
    names_to = "parameter",
    values_to = "estimate"
  ) |> 
  group_by(parameter) |> 
  summarize(
    ci_lower = quantile(estimate, 0.025),
    ci_upper = quantile(estimate, 0.975)
  ) |> 
  knitr::kable(col.names = c("Parameter","CI Lower Bound", "CI Upper Bound"),
               digits = 3)
```

| Parameter | CI Lower Bound | CI Upper Bound |
|:----------|---------------:|---------------:|
| log_beta  |         -8.612 |          -4.60 |
| r.squared |          0.889 |           0.94 |
